<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Henry&apos;s Portfolio</title>
    <description>M.S. in Robotics @ Northwestern University</description>
    <link>henryburon.github.io/</link>
    <atom:link href="henryburon.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 12 Jan 2024 02:06:13 -0600</pubDate>
    <lastBuildDate>Fri, 12 Jan 2024 02:06:13 -0600</lastBuildDate>
    <generator>Jekyll v3.9.2</generator>
    
      <item>
        <title>Polyglotbot: A 7 DoF Robot Arm that Writes Translated Text and Speech</title>
        <description>&lt;p&gt;ROS2, MoveIt!, RViz&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/f93vAm1NstA?si=q7lfAgRUdSKHukga&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Develop a ROS2 package that allows a 7 DoF Framka Emika Panda robotic arm to write translated text and speech on a whiteboard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Team Members&lt;/strong&gt;: Allen Liu, Kassidy Shedd, Megan Black, Damien Koh&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/ME495-EmbeddedSystems/final-project-dkoh555&quot;&gt;https://github.com/ME495-EmbeddedSystems/final-project-dkoh555&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;My primary responsibilities for this project included:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Creating the &lt;em&gt;apriltags&lt;/em&gt;  and &lt;em&gt;speech&lt;/em&gt; packages&lt;/li&gt;
  &lt;li&gt;Working with the MoveIt! package to help convert waypoints to movement&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;apriltags&lt;/em&gt; Package&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The purpose of this package is to localize the AprilTags on the whiteboard, transform their 3D-locations into the robot’s base frame (panda_link0), and publish these coordinates so they can be accessed by the node used for movement.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GetAprilTags&lt;/em&gt; node in the package creates a static transformation that links the camera to robot base, looks up transforms between the tags and camera, constructs transformation matrices from Quaternions, extracts the coordinate values, and publishes them using a custom message type.&lt;/p&gt;

&lt;p&gt;In the image below, the AprilTags have been localized and transformed into the robot’s frame.
&lt;img src=&quot;/assets/images/localize_tags.png&quot; alt=&quot;Localize Tags&quot; /&gt;&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;speech&lt;/em&gt; Package&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The purpose of this package is to provide the speech-to-text functionality, as an alternative to the default text-to-text.&lt;/p&gt;

&lt;p&gt;The node, &lt;em&gt;ListenSpeech&lt;/em&gt;, is triggered by a service call. The package makes use of PyAudio and the speech_recognition library. By default, it translates the spoken language to English, but this can be changed with a different language code.&lt;/p&gt;

&lt;p&gt;The service call activates the &lt;em&gt;LISTENING&lt;/em&gt; state, which listens, then continues the node’s pipeline in the &lt;em&gt;RECOGNIZING&lt;/em&gt; state.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;State&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LISTENING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Microphone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# detects presence of external microphone
&lt;/span&gt;                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Say something...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recognizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjust_for_ambient_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# adjusts for ambient noise
&lt;/span&gt;                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recognizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# by default, it listens until it detects a pause
&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;State&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RECOGNIZING&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Waypoints to Movement&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Each letter that the robot writes begins as a series of waypoints (2D-coordinates) that must be converted into physical movement by the robot arm. To do this, we created a custom &lt;em&gt;move_robot&lt;/em&gt; Python API to plan and execute paths using the MoveIt! MoveGroup and ExecuteTrajectory Action Clients, respectively. We make use of MoveIt!’s &lt;em&gt;compute_cartesian_path&lt;/em&gt; service to follow a smooth and stable path.&lt;/p&gt;

&lt;p&gt;See the &lt;em&gt;move_robot&lt;/em&gt; API: &lt;a href=&quot;https://github.com/henryburon/move-robot/tree/main/move_robot&quot;&gt;&lt;em&gt;move_robot&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The API plans collision-free paths and allows the user to send the end-effector to a desired configuration with code as simple as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# self.comm_count is incremented after each successful execution of a position command
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# desired location
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ori_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Quaternion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# desired orientation
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;robot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_and_execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                    &lt;span class=&quot;n&quot;&gt;quat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ori_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How it Works&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Calibrate&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The robot uses an Intel Realsense camera to acquire the location of three AprilTags, which, together, are used to constrain the plane of the whiteboard.&lt;/li&gt;
      &lt;li&gt;The distance and orientation of the whiteboard is made public through a custom message type on a ROS2 topic.&lt;/li&gt;
      &lt;li&gt;Using the known transformation between the camera link and the robot arm’s base link, the updated configuration (Pose) of the whiteboard is extrapolated into the frame of reference of the end-effector and is used to position the pen when writing.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Detect Text&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;By default, the robot then enters the “Detecting” state, in which it waits for a human to enter the frame and start writing on the whiteboard.&lt;/li&gt;
      &lt;li&gt;Using the YOLOv8 deep learning model, the robot recognizes once a human has entered the frame and then left, after which the next step commences.&lt;/li&gt;
      &lt;li&gt;The robot then uses the PaddleOCR library to detect the text written on the whiteboard and pass it along, without any processing, as a string. The desired language is written on the whiteboard as a short language code above the unknown word(s) (e.g. “en” for English).&lt;/li&gt;
      &lt;li&gt;Alternatively, instead of writing a word on the whiteboard, the user could call the &lt;em&gt;speech&lt;/em&gt; service which activates the microphone as an input. The user then simply speaks out loud, and their word(s) are passed along as a string from there. By default, the desired language for speech is set to English.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translate Text&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The next stage in the pipeline is the &lt;em&gt;translator&lt;/em&gt; node which takes in both the desired language and the string of the unknown word(s).&lt;/li&gt;
      &lt;li&gt;The translation node uses the Google Translate API and can translate to and from 50+ languages.&lt;/li&gt;
      &lt;li&gt;This step outputs a fully-translated string of text.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Text to Waypoints&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The &lt;em&gt;string2waypoints&lt;/em&gt; node uses matplotlib to convert each character to a series of waypoints–passed along as a series of &lt;em&gt;Point&lt;/em&gt; messages–which can then be followed by the robot arm.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Waypoints to Movement&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Finally, the robot uses the &lt;em&gt;write_letters&lt;/em&gt; package to convert the waypoints to movement and draw the letters on the board.&lt;/li&gt;
      &lt;li&gt;This package makes use of our custom &lt;em&gt;move_robot&lt;/em&gt; Python API to plan and execute robot arm paths using the MoveIt! MoveGroup and ExecuteTrajectory Action Clients, respectively.&lt;/li&gt;
      &lt;li&gt;We make use of MoveIt!’s &lt;em&gt;compute_cartesian_path&lt;/em&gt; service so as to follow a more direct and stable path when writing the letters, as opposed to &lt;em&gt;compute_ik&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/the_robot_is_cool.jpeg&quot; alt=&quot;The Robot Is Cool&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Dec 2023 00:00:00 -0600</pubDate>
        <link>henryburon.github.io/polyglotbot/</link>
        <guid isPermaLink="true">henryburon.github.io/polyglotbot/</guid>
        
        
        <category>ROS2</category>
        
        <category>MoveIt!</category>
        
        <category>RViz</category>
        
      </item>
    
      <item>
        <title>KUKA youBot Mobile Manipulation</title>
        <description>&lt;p&gt;Robotic Manipulation, Python, CoppeliaSim&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/ZHFO4J9itbI?si=82YA8ILq-O1_tx1X&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Write a program that automatically plans a trajectory for a KUKA youBot mobile manipulator as it grasps a block and places it in a desired location.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/henryburon/mobile-manipulation&quot;&gt;https://github.com/henryburon/mobile-manipulation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I wrote three functions to complete this project, which, when combined, generate the final trajectory:
    &lt;ul&gt;
      &lt;li&gt;TrajectoryGenerator&lt;/li&gt;
      &lt;li&gt;FeedbackControl&lt;/li&gt;
      &lt;li&gt;NextState&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TrajectoryGenerator&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Generates the reference trajectory for the end-effector frame {e}.&lt;/li&gt;
      &lt;li&gt;Consists of eight concatenated trajectory segments:
        &lt;ul&gt;
          &lt;li&gt;A trajectory to move the gripper from its initial configuration to a “standoff” configuration a few cm above the block.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper down to the grasp position&lt;/li&gt;
          &lt;li&gt;Closing of the gripper.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper back up to the “standoff” configuration.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper to a “standoff” configuration above the final configuration.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper to the final configuration of the object.&lt;/li&gt;
          &lt;li&gt;Opening of the gripper.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper back to the “standoff” configuration.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: A representation of the N configurations of the end-effector along the entire eight-segment reference trajectory.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FeedbackControl&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Calculates the kinematic task-space feedforward plus feedback control law.
 &lt;img src=&quot;/assets/images/feedback_control.png&quot; alt=&quot;Feedback Control&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: The commanded end-effector twist V expressed in the end-effector frame {e}.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NextState&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Calculates the robot’s configuration at the next time-step using first-order Euler-step.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: A 12-vector representing the configuration of the robot time Δt later.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Combine Functions&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Finally, I integrated these three functions, provided the necessary initial and desired final configurations, and generated a .csv file to simulate the KUKA youBot’s movement in CoppeliaSim.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Results&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I finished by generating three trajectories:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Best&lt;/strong&gt;: A well-tuned feedforward-plus-PI controller. Error quickly converged to zero. First clip in the YouTube video.
  &lt;img src=&quot;/assets/images/best_run.png&quot; alt=&quot;Best Run&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Overshoot&lt;/strong&gt;: A less-well-tuned feedforward-plus-PI controller. Error takes longer to converge to zero.
  &lt;img src=&quot;/assets/images/overshoot_run.png&quot; alt=&quot;Overshoot Run&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;newTask&lt;/strong&gt;: Different initial and final configurations of the block. Second clip in the YouTube video.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 25 Nov 2023 00:00:00 -0600</pubDate>
        <link>henryburon.github.io/mobile-manipulation/</link>
        <guid isPermaLink="true">henryburon.github.io/mobile-manipulation/</guid>
        
        
        <category>Robotic Manipulation</category>
        
        <category>Python</category>
        
        <category>CoppeliaSim</category>
        
      </item>
    
      <item>
        <title>Machine Learning Emotion Classification</title>
        <description>&lt;p&gt;Machine Learning, Image Processing, Feature Extraction&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/CJrl6uvsziY?si=CeYyhI1T_yrKK-e9&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Create a robust emotion classification pipeline by developing a machine learning algorithm capable of classifying facial images based on their depicted emotion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/henryburon/ml-emotion-classification&quot;&gt;https://github.com/henryburon/ml-emotion-classification&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Load, Process, and Store Images&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;I loaded several thousand 48x48 grayscale training and testing images from seven different emotion categories, converted them to numpy arrays, and normalized the pixel values.&lt;/li&gt;
      &lt;li&gt;Kaggle Dataset: &lt;a href=&quot;https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer&quot;&gt;https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Apply Histogram of Oriented Gradients (HOG) Feature Extraction&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Facial images are heavily influenced by subtle changes in facial shape and the presence of edges (e.g. raised eyebrows, open mouth). HOG excels at capturing these features by computing the oriented gradients within localized regions of the image. In addition, HOG is known for its robustness in variations in lighting and contrast, which are common challenges in image processing. Finally, implementing HOG is relatively efficient compared to other sophisticated feature extraction methods, which was important in my project, considering the number of images I needed to process.
 &lt;img src=&quot;/assets/images/hog_image_example.png&quot; alt=&quot;HOG Image Example&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assign Labels to Data:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I assigned seven labels to the data, corresponding to seven different emotion classes:
 [0] angry, [1] happy, [2] neutral, [3] sad, [4] disgusted, [5] fearful, [6] surprised&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Train Model&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;I trained the linear regression model, made predictions, obtained the classification report, and saved the trained model for further use, achieving an accuracy of up to 77%.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improving Model Accuracy&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;I took several steps to increase the accuracy of the model by improving the feature extraction process. For example, through experimentation, I optimized the number of orientations, block size, and cell size, all with the goal of enhancing the granularity and thereby descriptive power of the extracted features. I also investigated different normalization techniques, such as min-max scaling, although this did not discernably affect the results.&lt;/li&gt;
      &lt;li&gt;Looking ahead, instead of engineering the features myself (i.e. HOG), I intend to try more feature learning techniques–automatic feature engineering–as this would empower the machine to autonomously learn more effective representations from the raw data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 15 Nov 2023 00:00:00 -0600</pubDate>
        <link>henryburon.github.io/ml_emotion_classification/</link>
        <guid isPermaLink="true">henryburon.github.io/ml_emotion_classification/</guid>
        
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Image Processing</category>
        
      </item>
    
      <item>
        <title>Computer Vision-Controlled Robot Arm</title>
        <description>&lt;p&gt;OpenCV, Python, Transforms&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/pturk2xscaA?si=CLWhMrOFVhrZ33sk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Use the PincherX 100 robot arm to autonomously grab a purple pen.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&quot;https://github.com/henryburon/pen-thief&quot;&gt;https://github.com/henryburon/pen-thief&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Detect Location of the Purple Pen&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;First, I used the RGB image from an Intel RealSense camera to create an HSV mask that filtered out every color except purple.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identify Contour and Calculate Centroid&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I added contours around the selected pixels and found the 2D coordinate of the centroid of the largest contour, which I assumed to be the pen.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Align the Images&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I then aligned the camera’s Depth Map with the RGB Image and found the pen’s 3D coordinates in the camera’s reference frame.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transform to Robot Frame&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I converted these coordinates to be in the robot arm’s frame, given its 90° rotation and fixed offset.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Command the End-Effector&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Finally, I used the InterbotixManipulatorXS Python package to move the end-effector and gripper to the desired coordinate and position.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 Sep 2023 00:00:00 -0500</pubDate>
        <link>henryburon.github.io/pen_thief/</link>
        <guid isPermaLink="true">henryburon.github.io/pen_thief/</guid>
        
        
        <category>OpenCV</category>
        
        <category>Python</category>
        
        <category>Transforms</category>
        
      </item>
    
      <item>
        <title>Rapidly-Exploring Random Tree (RRT)</title>
        <description>&lt;p&gt;Path Planning, Python, RRT&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/x5AmgLBkSXQ?si=LW6sG8rTdrXVrDQt&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Implement the RRT path-planning algorithm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/henryburon/path-planning/tree/main/path_planning/RRT&quot;&gt;https://github.com/henryburon/path-planning/tree/main/path_planning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;This is an implementation of the Rapidly-Exploring Random Tree (RRT), a fundamental path planning algorithm in robotics.&lt;/p&gt;

&lt;p&gt;An RRT consists of a set of vertices, which represent configurations in some domain D and edges, which connect two vertices. The algorithm randomly builds a tree in such a way that, as the number of vertices and &lt;em&gt;n&lt;/em&gt; increases to ∞, the vertices are uniformly distributed across the domain D.&lt;/p&gt;

&lt;p&gt;In this implementation, the RRT algorithm uses two key components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Tree Data Structure&lt;/strong&gt;: The RRT algorithm maintains a tree data structure to explore and represent the configuration space efficiently. Each node in the tree corresponds to a specific vertex, and edges between nodes represent feasible transitions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;State Machine&lt;/strong&gt;: To control the decision-making process during tree expansion and exploration, I employed a state machine to guide the algorithm’s behavior.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The inputs are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;q_init&lt;/strong&gt;: initial configuration&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;K&lt;/strong&gt;: max vertices in the RRT&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;delta&lt;/strong&gt;: incremental distance between vertices&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;domain&lt;/strong&gt;: the planning domain (default 100x100)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The algorithm begins by randomly creating the obstacles, initializing the goal (and checking it’s in an acceptable location), and initializing the matplotlib plot. It then begins the main process, which repeats up to &lt;strong&gt;K&lt;/strong&gt; times.&lt;/p&gt;

&lt;p&gt;A goal is considered acceptable if it is near the edge of the domain and not inside an obstacle:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initialize_goal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;acceptable_goal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acceptable_goal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;check1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;check2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_goal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_goal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_goal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_goal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# goal must be near edge of domain (so RRT has to search a little)
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;check1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;circle&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;circles_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# goal must not be inside an obstacle
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;coord&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;coordinate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_goal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_goal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;check2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;acceptable_goal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# proceed with current goal
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Every iteration, the algorithm:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Finds a random configuration (random coordinate within the domain).&lt;/li&gt;
  &lt;li&gt;Identifies the closest existing vertex.&lt;/li&gt;
  &lt;li&gt;Calculates the direction of the random configuration.&lt;/li&gt;
  &lt;li&gt;Plans a new vertex delta (1) in the direction of the random configuration.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Checks if the planned vertex collides with an obstacle, and either returns to Step 1 if needed, or continues if there will not be a collision.&lt;/p&gt;

    &lt;p&gt;Collision checking is implemented as follows:&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;CHECK_COLLISIONS&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
             &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obstacle&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;circles_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# check each obstacle
&lt;/span&gt;                 &lt;span class=&quot;n&quot;&gt;coord&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obstacle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;coordinate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# get center coordinate of obstacle
&lt;/span&gt;                 &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                 &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obstacle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# if new configuration is within obstacle, get new random coordinate
&lt;/span&gt;                     &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;RANDOM_CONFIG&quot;&lt;/span&gt;
                     &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
                    
                 &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                     &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;CHECK_FOR_GOAL&quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# continue to next stage if no collision detected
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Checks if the goal can be spotted (straight line, no obstacles) from the newest vertex. If so, a flag is activated and the algorithm ends.&lt;/li&gt;
  &lt;li&gt;Updates the tree with the newest child vertex.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The RRT is then animated with matplotlib.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rrt1.png&quot; style=&quot;width: 750px; height: auto; margin: 0 auto; display: block;&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 10 Sep 2023 00:00:00 -0500</pubDate>
        <link>henryburon.github.io/path_planning/</link>
        <guid isPermaLink="true">henryburon.github.io/path_planning/</guid>
        
        
        <category>Path Planning</category>
        
        <category>Python</category>
        
        <category>RRT</category>
        
      </item>
    
      <item>
        <title>Unmanned Electric Race Boat</title>
        <description>&lt;p&gt;ArduPilot, Electronics, Autonomous Systems&lt;/p&gt;

&lt;iframe src=&quot;https://www.facebook.com/plugins/video.php?height=314&amp;amp;href=https%3A%2F%2Fwww.facebook.com%2Fnavalengineers%2Fvideos%2F256584973783468%2F&amp;amp;show_text=false&amp;amp;width=560&amp;amp;t=0&quot; width=&quot;100%&quot; height=&quot;460&quot; style=&quot;border:none;overflow:hidden&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; allow=&quot;autoplay; clipboard-write; encrypted-media; picture-in-picture; web-share&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;Video provided by the American Society of Naval Engineers.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;For my undergraduate Capstone project at William &amp;amp; Mary, I led a team in building a boat from scratch to compete in the Unmanned Division of the 2023 Promoting Electric Propulsion for Small Craft (PEP) competition, a 5-mile endurance race. Our fully electric boat placed 3rd against universities from across the country.&lt;/p&gt;

&lt;p&gt;Our catamaran-style differential-drive boat, fondly named the &lt;em&gt;Colonial Cruiser&lt;/em&gt;, ran ArduPilot Rover on a Pixhawk Cube Orange and was controlled via radio during the competition, though it was capable of autonomous navigation during controlled tests.&lt;/p&gt;

&lt;p&gt;As the Team and Electrical Lead, I focused on the autonomous navigation and electric propulsion systems of the boat, though I had a hand in everything from hull design to the power distribution systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Team Members&lt;/strong&gt;: Owen Darcy, Ethan Chang, Shamsullah Ahmadzai&lt;br /&gt;
&lt;strong&gt;Advisor&lt;/strong&gt;: Jonathan Frey&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ardupilot.jpg&quot; alt=&quot;ArduPilot&quot; /&gt;
ArduPilot Ground Station setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/final_touches.jpg&quot; alt=&quot;Final Touches&quot; /&gt;
Final touches before the competition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lake_test.jpg&quot; alt=&quot;Lake Test&quot; /&gt;
Testing on the lake.&lt;/p&gt;
</description>
        <pubDate>Sun, 25 Jun 2023 00:00:00 -0500</pubDate>
        <link>henryburon.github.io/unmanned_electric_boat/</link>
        <guid isPermaLink="true">henryburon.github.io/unmanned_electric_boat/</guid>
        
        
        <category>ArduPilot</category>
        
        <category>Electronics</category>
        
        <category>Autonomous Systems</category>
        
      </item>
    
      <item>
        <title>About Me</title>
        <description>&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/headshot.jpeg&quot; style=&quot;width: 350px; height: auto; margin: 0 auto; display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hello! I am a current Master’s in Robotics student at Northwestern University in Evanston, IL. My background includes practical experience writing code for robotic manipulation and navigation, developing ROS2 packages, processing images and extracting features for machine learning, and a variety of other software engineering work in a Linux environment. I take pride in my ability to write robust and reliable code in Python and C/C++.
&lt;br /&gt;
&lt;br /&gt;
Some of my notable work includes co-developing a ROS2 package for motion planning on a 7-DoF robot arm, programming a robot to autonomously explore a simulated environment in Gazebo, writing software to plan a trajectory for a mobile robot’s end-effector, building a quadcopter from scratch, and using computer vision to control a robot arm as it identifies and grasps objects.
&lt;br /&gt;
&lt;br /&gt;
I am passionate about autonomous systems, computer vision, and mobile robotics, and I am currently seeking an internship or full-time position in robotics and software engineering.&lt;/p&gt;

&lt;h3 id=&quot;education&quot;&gt;&lt;a style=&quot;color: white; font-size:40px; font-family: &apos;Times New Roman&apos;, Times, serif;&quot;&gt;Education&lt;/a&gt;&lt;/h3&gt;

&lt;div style=&quot;display: flex; align-items: center; color: white;&quot;&gt;
    &lt;img src=&quot;/assets/images/nu.jpeg&quot; style=&quot;width: 75px; height: auto; margin-right: 10px;&quot; /&gt;
    &lt;div&gt;
        &lt;i&gt;Sep. 2023 - Dec. 2024&lt;/i&gt; &lt;br /&gt;
        &lt;b&gt;NU | M.S. in Robotics&lt;/b&gt;
    &lt;/div&gt;
    &lt;!-- Additional image placed here --&gt;
    &lt;img src=&quot;/assets/images/wm.jpeg&quot; style=&quot;width: 75px; height: auto; margin-left: 20px;&quot; /&gt;
    &lt;div style=&quot;margin-left: 10px;&quot;&gt; &lt;!-- Adjusted margin-left for the text div --&gt;
        &lt;i&gt;Sep. 2019 - May 2023&lt;/i&gt; &lt;br /&gt;
        &lt;b&gt;W&amp;amp;M | B.S. in Engineering Physics&lt;/b&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;&lt;a style=&quot;color: white; font-size:40px; font-family: &apos;Times New Roman&apos;, Times, serif;&quot;&gt;Contact&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;To contact me, please email: &lt;a href=&quot;mailto:henryburon2024@u.northwestern.edu&quot;&gt;henryburon2024@u.northwestern.edu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Linkedin: &lt;a href=&quot;https://www.linkedin.com/in/henryburon/&quot;&gt;https://www.linkedin.com/in/henryburon/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=&quot;https://github.com/henryburon&quot;&gt;https://github.com/henryburon&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;resume&quot;&gt;&lt;a style=&quot;color: white; font-size:40px; font-family: &apos;Times New Roman&apos;, Times, serif;&quot;&gt;Resume&lt;/a&gt;&lt;/h3&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;iframe src=&quot;_pages/HenryBuronResume.pdf&quot; width=&quot;100%&quot; height=&quot;1200px&quot; style=&quot;display: inline-block;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 14 Mar 2001 00:00:00 -0600</pubDate>
        <link>henryburon.github.io/HenryBuron_About</link>
        <guid isPermaLink="true">henryburon.github.io/HenryBuron_About</guid>
        
        
      </item>
    
  </channel>
</rss>
