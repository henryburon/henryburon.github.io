<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Henry&apos;s Portfolio</title>
    <description>M.S. in Robotics @ Northwestern University</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 12 Dec 2024 03:44:23 -0600</pubDate>
    <lastBuildDate>Thu, 12 Dec 2024 03:44:23 -0600</lastBuildDate>
    <generator>Jekyll v3.9.2</generator>
    
      <item>
        <title>Drone-Based Delivery System</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;ROS2, LoRa, Embedded Systems&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;90%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/1bbPotnd48Q?si=MclOsE42RdgTv6PT&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;I built a drone-based delivery system that delivers a package via tether. The package–in the form of a small mobile robot–stabilizes itself during descent, and has the ability to release itself from the tether and drive along the ground towards its destination.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/situational_diagram.png&quot; width=&quot;550&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GitHub Source Code:&lt;/strong&gt; &lt;a href=&quot;https://github.com/henryburon/drone-delivery&quot;&gt;https://github.com/henryburon/drone-delivery&lt;/a&gt;&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#system&quot;&gt;System&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#drone-and-winch&quot;&gt;Drone and Winch&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mobile-package-robot&quot;&gt;Mobile Package Robot&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#base-station&quot;&gt;Base Station&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#communication-architecture&quot;&gt;Communication Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#design&quot;&gt;Design&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#acknowledgements&quot;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;system&quot;&gt;System&lt;/h2&gt;

&lt;p&gt;Quickly delivering small quantities of medical or emergency supplies over distance can be the difference between life or death. Drones are uniquely capable of handling this task. However, those with sufficient payload capacity are often large, dangerous in crowded areas, and ill-suited for operation in constrained environments.&lt;/p&gt;

&lt;p&gt;My project aims to address these challenges by offering a safer and more adaptable solution.&lt;/p&gt;

&lt;p&gt;The system consists of three main parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Drone and winch&lt;/li&gt;
  &lt;li&gt;Mobile package robot&lt;/li&gt;
  &lt;li&gt;Base station&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;drone-and-winch&quot;&gt;Drone and Winch&lt;/h4&gt;

&lt;p&gt;My system uses a heavy-payload drone, along with a custom-built winch. The drone was built by &lt;a href=&quot;https://marnonel6.github.io/projects/0-autonomous-px4-drone&quot; target=&quot;_blank&quot;&gt;Marno Nel&lt;/a&gt;. The drone’s size (specifically, the space underneath) and lifting capacity provide the primary constraints for this project.&lt;/p&gt;

&lt;p&gt;The winch, fixed to the underbelly of the drone, allows the robot to be deployed–lowered with a tether–from approximately 10m in the air. The winch system is controlled and powered by the drone’s existing components, however it communicates with the base station via an added LoRa module.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/winch_cad.png&quot; width=&quot;500&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;mobile-package-robot&quot;&gt;Mobile Package Robot&lt;/h4&gt;

&lt;p&gt;The mobile robot carries the medical supplies. It is lowered from the drone by tether, and can detach itself from this tether without human assistance. Once on the ground, it can drive closer to the target, taking advantage of its small form factor to enter constrained and crowded environments. The laterally-located propellers can be used to stabilize itself during descent, allowing the robot to safely deliver the medical supplies in a range of environments. The robot communicates via LoRa. The robot was built from scratch.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/droid_closed.jpg&quot; width=&quot;45%&quot; /&gt;
   &lt;img src=&quot;/assets/images/droid_open.jpg&quot; width=&quot;45%&quot; /&gt;
   &lt;br /&gt;
   &lt;em&gt;Mobile package robot&lt;/em&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/gifs/tether_mechanism.gif&quot; width=&quot;500&quot; /&gt;
   &lt;br /&gt;
   &lt;em&gt;Tether mechanism in action&lt;/em&gt;
&lt;/p&gt;

&lt;h4 id=&quot;base-station&quot;&gt;Base Station&lt;/h4&gt;

&lt;p&gt;The base station simply consisted of a LoRa module connected to a computer. It allows the operator to receive status updates in real-time and send commands.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/dd_base_station.jpg&quot; width=&quot;500&quot; /&gt;
   &lt;br /&gt;
   &lt;em&gt;Base Station (R)&lt;/em&gt;
&lt;/p&gt;

&lt;h2 id=&quot;communication-architecture&quot;&gt;Communication Architecture&lt;/h2&gt;

&lt;p&gt;Reliable communication over distance is critical for a drone-based robotic delivery system.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/comm_diag_1.png&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;LoRa, short for Long Range, is a low-power, wide-area network communication protocol designed for wireless communication over long distances.&lt;/p&gt;

&lt;p&gt;It operates in sub-gigahertz radio frequency bands (i.e. 433 MHz, 915 MHz) and is well-suited for low-power, low-throughput applications. LoRa achieves this using chirp spread spectrum (CSS) modulation, a frequency spreading method that allows for better resistance to interference and signal degradation over long distances. For these reasons, I used &lt;a href=&quot;https://www.adafruit.com/product/3072&quot;&gt;LoRa modules&lt;/a&gt; to facilitate the communication between the different parts of the system.&lt;/p&gt;

&lt;h2 id=&quot;design&quot;&gt;Design&lt;/h2&gt;

&lt;p&gt;I built the mobile package robot, the winch system, and the base station from scratch.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/elec_diag_1.png&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The winch system electronics consist of a high-torque servo motor and LoRa module. It is connected to the drone’s power supply.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks to Matt, Davin, Shail, and Marno for your help in this project.&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Dec 2024 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/drone-delivery/</link>
        <guid isPermaLink="true">http://localhost:4000/drone-delivery/</guid>
        
        
        <category>ROS2</category>
        
        <category>LoRa</category>
        
        <category>Embedded Systems</category>
        
      </item>
    
      <item>
        <title>Internship at NASA Jet Propulsion Laboratory (JPL)</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;C++, ROS Noetic, OpenCV, Eigen&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/gifs/niosh.gif&quot; alt=&quot;NIOSH Robot&quot; width=&quot;90%&quot; /&gt;
   &lt;br /&gt;
   &lt;em&gt;Testing the robot in the Mars Yard.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: JPL cleared this content for public release.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;See story on the MSR website: &lt;a href=&quot;https://www.mccormick.northwestern.edu/robotics/inside-our-program/stories/2024/navigating-hazardous-terrain-at-nasas-jet-propulsion-laboratory.html&quot; target=&quot;_blank&quot;&gt;Navigating Hazardous Terrain at NASA’s Jet Propulsion Laboratory&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;I worked as a Robotics Software Intern at NASA’s Jet Propulsion Laboratory in the Robotic Mobility group (347F) during Summer 2024.&lt;/p&gt;

&lt;p&gt;I primarily developed ROS C++ packages to support the control of the NIOSH robot, a mobile robot designed to operate in the hazardous terrain of coal mines.&lt;/p&gt;

&lt;p&gt;The robot itself is rather unique–it’s made up of two rigid bodies that are joined by a single linkage. The connection points are actuated, meaning the robot can change its kinematic configuration in response to its environment or a specific task. This enables the robot to operate in configurations like side-by-side or leader-follower, each of which has different advantages such as increased stability or a slimmer profile.&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h1 id=&quot;projects&quot;&gt;Projects&lt;/h1&gt;

&lt;h4 id=&quot;imu-based-rollover-risk-detection&quot;&gt;IMU-Based Rollover-Risk Detection&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/gifs/rollover_risk.gif&quot; alt=&quot;Rollover Risk Detection&quot; width=&quot;40%&quot; /&gt;
   &lt;img src=&quot;/assets/images/rollover_risk.png&quot; alt=&quot;Rollover Risk Detection&quot; width=&quot;40%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;I developed a simulation-based empirical method to asses the robot’s risk of tipping in any given state, where the state is defined by its kinematic configuration and the tilt of each body.&lt;/p&gt;

&lt;p&gt;The algorithm models the &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_polygon&quot; target=&quot;_blank&quot;&gt;support polygon&lt;/a&gt; of the robot, along with the ground-plane projection of the its center of mass (yellow circle). As the center of mass projection approaches the edge of the support polygon, it indicates a decrease in the robot’s stability.&lt;/p&gt;

&lt;h4 id=&quot;robot-path-projection-lines&quot;&gt;Robot Path Projection Lines&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/gifs/proj_lines.gif&quot; alt=&quot;Robot Path Projection Lines&quot; width=&quot;49%&quot; /&gt;
   &lt;img src=&quot;/assets/images/proj_lines.png&quot; alt=&quot;Robot Path Projection Lines&quot; width=&quot;45%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;I developed an algorithm that generates path projection lines based on the camera’s intrinsic parameters and the robot’s configuration, with the lines representing the robot’s width on the ground plane. The system used a two-camera setup with a stitched video feed, and factors such as each camera’s yaw, pitch, field of view (FOV), and height are parameterized in the calculation.&lt;/p&gt;

&lt;h4 id=&quot;configuration-manager&quot;&gt;Configuration Manager&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/gifs/config_manager.gif&quot; alt=&quot;Robot Path Projection Lines&quot; width=&quot;60%&quot; /&gt;
   &lt;img src=&quot;/assets/images/niosh_kinematics.png&quot; alt=&quot;Robot Path Projection Lines&quot; width=&quot;29.2%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;I developed a configuration manager that allows the robot to efficiently change its kinematic configuration during operation.&lt;/p&gt;

&lt;p&gt;The algorithm abstracts the robot’s non-intuitive kinematics from the operator and prioritizes a small operational footprint during the transition.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/niosh_terrain.png&quot; alt=&quot;SLAM&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Sep 2024 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/jpl_internship/</link>
        <guid isPermaLink="true">http://localhost:4000/jpl_internship/</guid>
        
        
        <category>C++</category>
        
        <category>ROS Noetic</category>
        
        <category>OpenCV</category>
        
        <category>Eigen</category>
        
      </item>
    
      <item>
        <title>Computer Vision-Based Basketball Coach</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;OpenCV, Python, Computer Vision, Object Tracking&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;90%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/MW7uQ3kL7gM?si=L13EQ9G4aMUwJl2X&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;This project uses computer vision to analyze, score, and provide feedback on your basketball shot, effectively functioning as a virtual coach.&lt;/p&gt;

&lt;p&gt;The program tracks the ball’s trajectory and your body’s movement, comparing these to a ground truth (a “perfect” shot). It then calculates a performance score and generates a PDF report with specific feedback.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Teammate&lt;/strong&gt;: Srikanth Schelbert&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/henryburon/cv_basketball_trainer&quot;&gt;https://github.com/henryburon/cv_basketball_trainer&lt;/a&gt;&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#motion-tracking&quot;&gt;Motion Tracking&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basketball-tracking-algorithm&quot;&gt;Basketball Tracking Algorithm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#body-movement-tracking&quot;&gt;Body Movement Tracking&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-analysis-and-scoring&quot;&gt;Data Analysis and Scoring&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;motion-tracking&quot;&gt;Motion Tracking&lt;/h2&gt;

&lt;p&gt;The first step is to collect the data that is to be analyzed, compared, and scored.&lt;/p&gt;

&lt;h4 id=&quot;basketball-tracking-algorithm&quot;&gt;Basketball Tracking Algorithm&lt;/h4&gt;

&lt;p&gt;I was primarily responsible for developing the algorithm that identifies and tracks the basketball through the video.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Algorithm:&lt;/code&gt; apply color mask &lt;strong&gt;»&lt;/strong&gt; find contours &lt;strong&gt;»&lt;/strong&gt; score contours &lt;strong&gt;»&lt;/strong&gt; identify basketball&lt;/p&gt;

&lt;p&gt;Applying the HSV color mask and identifying the contours with OpenCV is a straightforward process. The objective is simply to reduce the amount of &lt;em&gt;potential basketball contours&lt;/em&gt; in the frame and therefore make the scoring process easier.&lt;/p&gt;

&lt;p&gt;The contours are scored on:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Size&lt;/li&gt;
  &lt;li&gt;Squareness&lt;/li&gt;
  &lt;li&gt;Distance to the basketball’s location in the previous frame&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each contour in the frame is graded on the above three characteristics, and each of those grades are weighted and summed up to create a single score for the contour. The contour with the highest score is assumed to be the basketball, and its coordinates are saved into the trajectory array.&lt;/p&gt;

&lt;div style=&quot;display: flex; justify-content: space-around;&quot;&gt;
    &lt;div&gt;
        &lt;img src=&quot;/assets/images/cv_bball_contours.png&quot; width=&quot;500&quot; /&gt;
        &lt;small&gt;Figure 1. Scored contour rectangles. Green outline indicates basketball. Red circle is location in previous frame.&lt;/small&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;img src=&quot;/assets/images/cv_bball_traj.png&quot; width=&quot;500&quot; /&gt;
        &lt;small&gt;Figure 2. Sample collected basketball trajectory data.&lt;/small&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h4 id=&quot;body-movement-tracking&quot;&gt;Body Movement Tracking&lt;/h4&gt;

&lt;p&gt;We used MediaPipe to track the body during the shot.&lt;/p&gt;

&lt;p&gt;Specifically, we tracked the motion of the wrist and elbow, as their movement greatly influences a shot’s success.&lt;/p&gt;

&lt;p&gt;We combined this data with the basketball’s motion and were then able to identify the moment the ball was released from the player’s hands.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/cv_every_traj.png&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;
&lt;center&gt;&lt;small&gt;Figure 3. All trajectories plotted.&lt;/small&gt;&lt;/center&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;data-analysis-and-scoring&quot;&gt;Data Analysis and Scoring&lt;/h2&gt;

&lt;p&gt;In order to score the shot, we compared its similarity to a ground truth/perfect shot. If you’re curious, we compared it to &lt;a href=&quot;https://180shooter.com/why-everyone-should-shoot-like-steve-nash&quot;&gt;Steve Nash’s&lt;/a&gt; shot, a former professional NBA player.&lt;/p&gt;

&lt;p&gt;To evaluate the trajectory similarity between the user’s shot and Nash’s shot, we used Fast Dynamic Time Warping (FastDTW) and Procrustes analysis.&lt;/p&gt;

&lt;p&gt;FastDTW aligns two time-series data, even if they are not perfectly synchronized in time. This allowed us to compare the similarity of the two shots, regardless of how fast or slow they were taken.&lt;/p&gt;

&lt;p&gt;Procrustes analysis focuses on comparing the shape of the trajectories themselves, not how they were executed in time. It removes the differences in position, scale, and rotation, which is useful given that the videos are often taken with different frame sizes.&lt;/p&gt;

&lt;p&gt;The result is that these methods allow us to compare the general shape of the trajectory and not worry about differences in timing or video quality.&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The result of the program is an automatically generated PDF report with personalized feedback.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/cv_bball_report.png&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;
&lt;center&gt;&lt;small&gt;Figure 4. Sample PDF report.&lt;/small&gt;&lt;/center&gt;
</description>
        <pubDate>Wed, 10 Apr 2024 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/virtual_bball_trainer/</link>
        <guid isPermaLink="true">http://localhost:4000/virtual_bball_trainer/</guid>
        
        
        <category>OpenCV</category>
        
        <category>Python</category>
        
        <category>Computer Vision</category>
        
        <category>Object Tracking</category>
        
      </item>
    
      <item>
        <title>Mobile Exploration Robot with Auxiliary Drone</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;ROS2, Embedded Systems, Multi-Robot System, Autonomous Flight&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;90%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/72QHhtjNWzE?si=BSeyyCFr5hVDhiTT&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Terraflight&lt;/em&gt; is a ROS2-controlled mobile exploration robot built from the ground up. The custom-built rover carries a drone that can be deployed from the field during operation.&lt;/p&gt;

&lt;p&gt;The robot streams live video from both the rover and drone, and also uses a LiDAR module to perform SLAM and map its environment. The entire system is controlled and monitored from a base station.&lt;/p&gt;

&lt;p&gt;The drone is primarily teleoperated, but is able to autonomously re-land on the rover. The drone localizes the rover with AprilTags, after which the user is able to call the autonomous landing service with the joystick controller.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub Source Code&lt;/strong&gt;: &lt;a href=&quot;https://github.com/henryburon/terra-flight&quot;&gt;https://github.com/henryburon/terra-flight&lt;/a&gt;&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#structure&quot;&gt;Structure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#features&quot;&gt;Features&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#drone&quot;&gt;Drone&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#slam&quot;&gt;SLAM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#base-station&quot;&gt;Base Station&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#design&quot;&gt;Design&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#electrical&quot;&gt;Electrical&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mechanical&quot;&gt;Mechanical&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;structure&quot;&gt;Structure&lt;/h2&gt;

&lt;p&gt;The robot is built with a Raspberry Pi 4 and is fully controlled using ROS2 in Python.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/terraflight_control_architecture.png&quot; alt=&quot;control_architecture&quot; /&gt;
&lt;small&gt; Figure 1. Block diagram representing a simplified view of the robot’s control and communication architecture within the ROS2 framework.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Due to the computational limitations of the onboard Raspberry Pi 4, which functions as the robot’s central processing unit, the architecture is designed to delegate computation and data processing to the base station and reduce the Pi’s overall processing responsibilities when that is not possible. This necessity accounts for the low frequency of the robot’s camera feed (2 Hz).&lt;/p&gt;

&lt;p&gt;View the &lt;a href=&quot;https://github.com/henryburon/terra-flight&quot;&gt;source code&lt;/a&gt; for more information on the ROS2 packages that make up this project.&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;

&lt;h4 id=&quot;drone&quot;&gt;Drone&lt;/h4&gt;
&lt;p&gt;The rover carries a &lt;a href=&quot;https://store.dji.com/product/tello?vid=38421&quot;&gt;DJI Tello drone&lt;/a&gt; on its top platform throughout operation, and is capable of remotely deploying it from the field. The drone is teleoperated via joystick commands from the user, but is capable of autonomously re-landing on the rover after it locates the rover during flight.&lt;/p&gt;

&lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/not_located1.png&quot; width=&quot;255&quot; /&gt;
  &lt;img src=&quot;/assets/images/located1.png&quot; width=&quot;255&quot; /&gt; 
  &lt;img src=&quot;/assets/images/not_located2.png&quot; width=&quot;255&quot; /&gt;
&lt;/p&gt; --&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/three_images.png&quot; width=&quot;1000&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;small&gt;  Figure 2. Drone camera feed as it locates the rover.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The drone uses AprilTags on the right, left, and back of the chassis to localize the rover. Once spotted, the user is able to call the autonomous landing service which directs the drone to follow the most recent transform between itself and the rover, adjusting for the location of the specific tag it saw. The drone also displays the time since the last reading, and the update status bar trends towards red as the drone goes longer without an update.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/located2.png&quot; width=&quot;475&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;small&gt;  Figure 3. Drone locating the rover via the right tag.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The rover can be located from any of its three AprilTags, and the drone adjusts its autonomous re-landing flight plan to ensure it lands on the rover facing forward.&lt;/p&gt;

&lt;h4 id=&quot;slam&quot;&gt;SLAM&lt;/h4&gt;

&lt;p&gt;The robot uses a LiDAR module mounted on the top of the rover to perform 2D SLAM and estimate its pose as it creates a map of the environment.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/slam_hallway2.png&quot; width=&quot;675&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;small&gt; Figure 4. Map created in a hallway at Northwestern’s Tech Institute.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The robot uses SLAM Toolbox. The odometry tf frame is calculated based on wheel velocities.&lt;/p&gt;

&lt;h4 id=&quot;base-station&quot;&gt;Base Station&lt;/h4&gt;

&lt;p&gt;The base station, operated via joystick inputs, is the primary control hub issuing movement commands and processing incoming data. It provides a dynamic interface that offers real-time video stream from the rover and drone, while simultaneously showing the map being built as the robot navigates and explores its environment.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/base_station1.jpg&quot; width=&quot;645&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;small&gt; Figure 5. The base station during operation. Hardware includes a laptop, USB WiFi adapter, and joystick controller.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;As long as the base station is connected to both the Tello drone’s WiFi network and a network configured to facilitate ROS2 discovery, the robot can be operated anywhere.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/base_station2.png&quot; width=&quot;945&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;small&gt; Figure 6. Screenshot from the base station’s interface. Top left: Drone camera. Bottom left: Rover camera. Right: SLAM.&lt;/small&gt;&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;design&quot;&gt;Design&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Terraflight&lt;/em&gt; was built from scratch.&lt;/p&gt;

&lt;h4 id=&quot;electrical&quot;&gt;Electrical&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/electrical_diagram1.png&quot; width=&quot;545&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;small&gt; Figure 7. Electrical block diagram.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;14.8V main bus. The power system allows the user to power the Raspberry Pi 4 individually when providing software updates.&lt;/p&gt;

&lt;h4 id=&quot;mechanical&quot;&gt;Mechanical&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/mechanical_terraflight.png&quot; width=&quot;750&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;small&gt; Figure 8. Labeled image of &lt;em&gt;Terraflight&lt;/em&gt;. &lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The rover’s drive train was inspired by &lt;a href=&quot;https://github.com/nasa-jpl/open-source-rover/tree/master/mechanical&quot;&gt;NASA’s Open Source JPL Rover&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Mar 2024 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/terraflight/</link>
        <guid isPermaLink="true">http://localhost:4000/terraflight/</guid>
        
        
        <category>Python</category>
        
        <category>ROS2</category>
        
        <category>Embedded Systems</category>
        
        <category>Multi-Robot System</category>
        
        <category>Autonomous Flight</category>
        
      </item>
    
      <item>
        <title>Extended Kalman Filter SLAM from Scratch</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;C++, ROS2, Unit Testing, CMake&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slam_img.png&quot; alt=&quot;SLAM&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;I implemented an Extended Kalman filter (EKF) SLAM algorithm from scratch through several ROS2 packages and a custom C++ library.&lt;/p&gt;

&lt;p&gt;In the above image, there are three simulated turtlebots. The &lt;strong&gt;red&lt;/strong&gt; turtlebot represents the ground truth, the &lt;strong&gt;blue&lt;/strong&gt; is the odometry estimate, and the &lt;strong&gt;green&lt;/strong&gt; is the estimate with SLAM. As can be seen, the estimate with SLAM performs much better than the estimate with just odometry.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/ME495-Navigation/slam-project-henryburon&quot;&gt;https://github.com/ME495-Navigation/slam-project-henryburon&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;turtlelib&lt;/code&gt;&lt;/strong&gt; is a C++ library for handling SE(2) math and other turtlebot-related calculations.&lt;br /&gt;
&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nusim&lt;/code&gt;&lt;/strong&gt; is a ROS2 package that provides a simulated environment for the robots.&lt;br /&gt;
&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nuturtle_description&lt;/code&gt;&lt;/strong&gt; is a ROS2 package that dispalys turtlebot3 models in RViz.&lt;br /&gt;
&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nuturtle_control&lt;/code&gt;&lt;/strong&gt; controls the robot in simulation and the physical world.&lt;br /&gt;
&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nuslam&lt;/code&gt;&lt;/strong&gt; implements EKF SLAM to estimate the robot’s pose.&lt;/p&gt;

&lt;!-- ## Extended Kalman Filter SLAM

EKF SLAM estimates the robot&apos;s pose (position and orientation) as it constructs a map of its environment.

The algorithm works by --&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h2&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;turtlelib&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Designed to facilitate geometric computations and two-dimensional rigid body transformations. Provided functionalities include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Point, vector, and twist manipulation&lt;/li&gt;
  &lt;li&gt;SVG-based visualization&lt;/li&gt;
  &lt;li&gt;Operator overloading&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All functions are tested using the Catch2 unit test framework.&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nusim&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ROS2 C++ node that provides a simulated environment for the robot. Launches RViz and displays walls, obstacles, and a single red robot. User can configure launch parameters in basic_world.yaml.&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nuturtle_description&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Builds turtlebot models with imported meshes in URDF; displays in RViz. Option to display all four models, or just one.&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nuturtle_control&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Computes and publishes the odometry the turtlebot based on the joint states of its wheels. Controls movement of the turtlebot robot based on velocity commands and sensor data from the robot’s wheel encoders.&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nuslam&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Implements EKF SLAM for the turtlebot, a differential-drive robot. The node estimates the robot’s pose and the positions of obstacles in the environment by combining odometry data from the robot’s wheel encoders with simulated sensor data from obstacles in the environment.&lt;/p&gt;

&lt;p&gt;The estimates odometry, path, and obstacles are published for visualization.&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2024 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/turtlebot_slam/</link>
        <guid isPermaLink="true">http://localhost:4000/turtlebot_slam/</guid>
        
        
        <category>C++</category>
        
        <category>ROS2</category>
        
        <category>Unit Testing</category>
        
        <category>CMake</category>
        
      </item>
    
      <item>
        <title>Polyglotbot: A 7 DoF Robot Arm that Writes Translated Text and Speech</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;ROS2, Python, Motion Planning&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;90%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/f93vAm1NstA?si=q7lfAgRUdSKHukga&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Developed a ROS2 package that allows a 7 DoF Franka Emika Panda robotic arm to write translated text and speech on a whiteboard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Team&lt;/strong&gt;: Allen Liu, Kassidy Shedd, Megan Black, Damien Koh&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/ME495-EmbeddedSystems/final-project-dkoh555&quot;&gt;https://github.com/ME495-EmbeddedSystems/final-project-dkoh555&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My primary responsibilities for this project included:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Creating the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apriltags&lt;/code&gt;  and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech&lt;/code&gt; packages&lt;/li&gt;
  &lt;li&gt;Working with the MoveIt2 package to help convert waypoints to movement&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apriltags&lt;/code&gt; package&lt;/p&gt;

&lt;p&gt;The purpose of this package is to localize the AprilTags on the whiteboard, transform their 3D-locations into the robot’s base frame (panda_link0), and publish these coordinates so they can be accessed by the node used for movement.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GetAprilTags&lt;/em&gt; node in the package creates a static transformation that links the camera to robot base, looks up transforms between the tags and camera, constructs transformation matrices from Quaternions, extracts the coordinate values, and publishes them using a custom message type.&lt;/p&gt;

&lt;p&gt;In the image below, the AprilTags have been localized and transformed into the robot’s frame.
&lt;img src=&quot;/assets/images/localize_tags.png&quot; alt=&quot;Localize Tags&quot; /&gt;&lt;/p&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech&lt;/code&gt; package&lt;/p&gt;

&lt;p&gt;The purpose of this package is to provide the speech-to-text functionality, as an alternative to the default text-to-text.&lt;/p&gt;

&lt;p&gt;The node, &lt;em&gt;ListenSpeech&lt;/em&gt;, is triggered by a service call. The package makes use of PyAudio and the speech_recognition library. By default, it translates the spoken language to English, but this can be changed with a different language code.&lt;/p&gt;

&lt;p&gt;The service call activates the &lt;em&gt;LISTENING&lt;/em&gt; state, which listens, then continues the node’s pipeline in the &lt;em&gt;RECOGNIZING&lt;/em&gt; state.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;State&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LISTENING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Microphone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# detects presence of external microphone
&lt;/span&gt;                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Say something...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recognizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjust_for_ambient_noise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# adjusts for ambient noise
&lt;/span&gt;                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recognizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# by default, it listens until it detects a pause
&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;State&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RECOGNIZING&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Waypoints to Movement&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Each letter that the robot writes begins as a series of waypoints (2D-coordinates) that must be converted into physical movement by the robot arm. To do this, we created a custom &lt;em&gt;move_robot&lt;/em&gt; Python API to plan and execute paths using the MoveIt2 MoveGroup and ExecuteTrajectory Action Clients, respectively. We make use of MoveIt2’s &lt;em&gt;compute_cartesian_path&lt;/em&gt; service to follow a smooth and stable path.&lt;/p&gt;

&lt;p&gt;See the &lt;em&gt;move_robot&lt;/em&gt; API: &lt;a href=&quot;https://github.com/henryburon/move-robot/tree/main/move_robot&quot;&gt;&lt;em&gt;move_robot&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The API plans collision-free paths and allows the user to send the end-effector to a desired configuration with code as simple as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# self.comm_count is incremented after each successful execution of a position command
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# desired location
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ori_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Quaternion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# desired orientation
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;robot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_and_execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                    &lt;span class=&quot;n&quot;&gt;quat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ori_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;background-color: white; height: 1px;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How it Works&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Calibrate&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The robot uses an Intel Realsense camera to acquire the location of three AprilTags, which, together, are used to constrain the plane of the whiteboard.&lt;/li&gt;
      &lt;li&gt;The distance and orientation of the whiteboard is made public through a custom message type on a ROS2 topic.&lt;/li&gt;
      &lt;li&gt;Using the known transformation between the camera link and the robot arm’s base link, the updated configuration (Pose) of the whiteboard is extrapolated into the frame of reference of the end-effector and is used to position the pen when writing.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Detect Text&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;By default, the robot then enters the “Detecting” state, in which it waits for a human to enter the frame and start writing on the whiteboard.&lt;/li&gt;
      &lt;li&gt;Using the YOLOv8 deep learning model, the robot recognizes once a human has entered the frame and then left, after which the next step commences.&lt;/li&gt;
      &lt;li&gt;The robot then uses the PaddleOCR library to detect the text written on the whiteboard and pass it along, without any processing, as a string. The desired language is written on the whiteboard as a short language code above the unknown word(s) (e.g. “en” for English).&lt;/li&gt;
      &lt;li&gt;Alternatively, instead of writing a word on the whiteboard, the user could call the &lt;em&gt;speech&lt;/em&gt; service which activates the microphone as an input. The user then simply speaks out loud, and their word(s) are passed along as a string from there. By default, the desired language for speech is set to English.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translate Text&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The next stage in the pipeline is the &lt;em&gt;translator&lt;/em&gt; node which takes in both the desired language and the string of the unknown word(s).&lt;/li&gt;
      &lt;li&gt;The translation node uses the Google Translate API and can translate to and from 50+ languages.&lt;/li&gt;
      &lt;li&gt;This step outputs a fully-translated string of text.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Text to Waypoints&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The &lt;em&gt;string2waypoints&lt;/em&gt; node uses matplotlib to convert each character to a series of waypoints–passed along as a series of &lt;em&gt;Point&lt;/em&gt; messages–which can then be followed by the robot arm.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Waypoints to Movement&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Finally, the robot uses the &lt;em&gt;write_letters&lt;/em&gt; package to convert the waypoints to movement and draw the letters on the board.&lt;/li&gt;
      &lt;li&gt;This package makes use of our custom &lt;em&gt;move_robot&lt;/em&gt; Python API to plan and execute robot arm paths using the MoveIt2 MoveGroup and ExecuteTrajectory Action Clients, respectively.&lt;/li&gt;
      &lt;li&gt;We make use of MoveIt2’s &lt;em&gt;compute_cartesian_path&lt;/em&gt; service so as to follow a more direct and stable path when writing the letters, as opposed to &lt;em&gt;compute_ik&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/the_robot_is_cool.jpeg&quot; alt=&quot;The Robot Is Cool&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Dec 2023 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/polyglotbot/</link>
        <guid isPermaLink="true">http://localhost:4000/polyglotbot/</guid>
        
        
        <category>ROS2</category>
        
        <category>MoveIt2</category>
        
        <category>Python</category>
        
        <category>Motion Planning</category>
        
      </item>
    
      <item>
        <title>KUKA youBot Mobile Manipulation</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;Python, Robotic Manipulation, CoppeliaSim&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/ZHFO4J9itbI?si=82YA8ILq-O1_tx1X&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Write a program that automatically plans a trajectory for a KUKA youBot mobile manipulator as it grasps a block and places it in a desired location.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/henryburon/mobile-manipulation&quot;&gt;https://github.com/henryburon/mobile-manipulation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I wrote three functions to complete this project, which, when combined, generate the final trajectory:
    &lt;ul&gt;
      &lt;li&gt;TrajectoryGenerator&lt;/li&gt;
      &lt;li&gt;FeedbackControl&lt;/li&gt;
      &lt;li&gt;NextState&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TrajectoryGenerator&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Generates the reference trajectory for the end-effector frame {e}.&lt;/li&gt;
      &lt;li&gt;Consists of eight concatenated trajectory segments:
        &lt;ul&gt;
          &lt;li&gt;A trajectory to move the gripper from its initial configuration to a “standoff” configuration a few cm above the block.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper down to the grasp position&lt;/li&gt;
          &lt;li&gt;Closing of the gripper.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper back up to the “standoff” configuration.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper to a “standoff” configuration above the final configuration.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper to the final configuration of the object.&lt;/li&gt;
          &lt;li&gt;Opening of the gripper.&lt;/li&gt;
          &lt;li&gt;A trajectory to move the gripper back to the “standoff” configuration.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: A representation of the N configurations of the end-effector along the entire eight-segment reference trajectory.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FeedbackControl&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Calculates the kinematic task-space feedforward plus feedback control law.
 &lt;img src=&quot;/assets/images/feedback_control.png&quot; alt=&quot;Feedback Control&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: The commanded end-effector twist V expressed in the end-effector frame {e}.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NextState&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Calculates the robot’s configuration at the next time-step using first-order Euler-step.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: A 12-vector representing the configuration of the robot time Δt later.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Combine Functions&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Finally, I integrated these three functions, provided the necessary initial and desired final configurations, and generated a .csv file to simulate the KUKA youBot’s movement in CoppeliaSim.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Results&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I finished by generating three trajectories:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Best&lt;/strong&gt;: A well-tuned feedforward-plus-PI controller. Error quickly converged to zero. First clip in the YouTube video.
  &lt;img src=&quot;/assets/images/best_run.png&quot; alt=&quot;Best Run&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Overshoot&lt;/strong&gt;: A less-well-tuned feedforward-plus-PI controller. Error takes longer to converge to zero.
  &lt;img src=&quot;/assets/images/overshoot_run.png&quot; alt=&quot;Overshoot Run&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;newTask&lt;/strong&gt;: Different initial and final configurations of the block. Second clip in the YouTube video.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 25 Nov 2023 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/mobile-manipulation/</link>
        <guid isPermaLink="true">http://localhost:4000/mobile-manipulation/</guid>
        
        
        <category>Python</category>
        
        <category>Robotic Manipulation</category>
        
        <category>CoppeliaSim</category>
        
      </item>
    
      <item>
        <title>Unmanned Electric Race Boat</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;ArduPilot, Electronics, Autonomous Systems&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe src=&quot;https://www.facebook.com/plugins/video.php?height=314&amp;amp;href=https%3A%2F%2Fwww.facebook.com%2Fnavalengineers%2Fvideos%2F256584973783468%2F&amp;amp;show_text=false&amp;amp;width=560&amp;amp;t=0&quot; width=&quot;100%&quot; height=&quot;460&quot; style=&quot;border:none;overflow:hidden&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; allow=&quot;autoplay; clipboard-write; encrypted-media; picture-in-picture; web-share&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;Video provided by the American Society of Naval Engineers.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;For my undergraduate Capstone project at William &amp;amp; Mary, I led a team in building a boat from scratch to compete in the Unmanned Division of the 2023 Promoting Electric Propulsion for Small Craft (PEP) competition, a 5-mile endurance race. Our fully electric boat placed 3rd against universities from across the country.&lt;/p&gt;

&lt;p&gt;Our catamaran-style differential-drive boat, fondly named the &lt;em&gt;Colonial Cruiser&lt;/em&gt;, ran ArduPilot Rover on a Pixhawk Cube Orange and was controlled via radio during the competition, though it was capable of autonomous navigation during controlled tests.&lt;/p&gt;

&lt;p&gt;As the Team and Electrical Lead, I focused on the autonomous navigation and electric propulsion systems of the boat, though I had a hand in everything from hull design to the power distribution systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Team&lt;/strong&gt;: Owen Darcy, Ethan Chang, Shamsullah Ahmadzai&lt;br /&gt;
&lt;strong&gt;Advisor&lt;/strong&gt;: Jonathan Frey&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ardupilot.jpg&quot; alt=&quot;ArduPilot&quot; /&gt;
ArduPilot Ground Station setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/final_touches.jpg&quot; alt=&quot;Final Touches&quot; /&gt;
Final touches before the competition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lake_test.jpg&quot; alt=&quot;Lake Test&quot; /&gt;
Testing on the lake.&lt;/p&gt;
</description>
        <pubDate>Sun, 15 Oct 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/unmanned_electric_boat/</link>
        <guid isPermaLink="true">http://localhost:4000/unmanned_electric_boat/</guid>
        
        
        <category>ArduPilot</category>
        
        <category>Electronics</category>
        
        <category>Autonomous Systems</category>
        
      </item>
    
      <item>
        <title>Computer Vision-Controlled Robot Arm</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;Python, OpenCV, Transforms&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/pturk2xscaA?si=CLWhMrOFVhrZ33sk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Use the PincherX 100 robot arm to autonomously grab a purple pen.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&quot;https://github.com/henryburon/pen-thief&quot;&gt;https://github.com/henryburon/pen-thief&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Detect Location of the Purple Pen&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;First, I used the RGB image from an Intel RealSense camera to create an HSV mask that filtered out every color except purple.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identify Contour and Calculate Centroid&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I added contours around the selected pixels and found the 2D coordinate of the centroid of the largest contour, which I assumed to be the pen.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Align the Images&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I then aligned the camera’s Depth Map with the RGB Image and found the pen’s 3D coordinates in the camera’s reference frame.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transform to Robot Frame&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I converted these coordinates to be in the robot arm’s frame, given its 90° rotation and fixed offset.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Command the End-Effector&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Finally, I used the InterbotixManipulatorXS Python package to move the end-effector and gripper to the desired coordinate and position.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 Sep 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/pen_thief/</link>
        <guid isPermaLink="true">http://localhost:4000/pen_thief/</guid>
        
        
        <category>OpenCV</category>
        
        <category>Python</category>
        
        <category>Transforms</category>
        
      </item>
    
      <item>
        <title>Machine Learning Emotion Classification</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:rgb(0, 30, 80)&quot;&gt;Machine Learning, Python, Image Processing&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;441&quot; src=&quot;https://www.youtube.com/embed/CJrl6uvsziY?si=CeYyhI1T_yrKK-e9&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Create a robust emotion classification pipeline by developing a machine learning algorithm capable of classifying facial images based on their depicted emotion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&quot;https://github.com/henryburon/ml-emotion-classification&quot;&gt;https://github.com/henryburon/ml-emotion-classification&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Load, Process, and Store Images&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;I loaded several thousand 48x48 grayscale training and testing images from seven different emotion categories, converted them to numpy arrays, and normalized the pixel values.&lt;/li&gt;
      &lt;li&gt;Kaggle Dataset: &lt;a href=&quot;https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer&quot;&gt;https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Apply Histogram of Oriented Gradients (HOG) Feature Extraction&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Facial images are heavily influenced by subtle changes in facial shape and the presence of edges (e.g. raised eyebrows, open mouth). HOG excels at capturing these features by computing the oriented gradients within localized regions of the image. In addition, HOG is known for its robustness in variations in lighting and contrast, which are common challenges in image processing. Finally, implementing HOG is relatively efficient compared to other sophisticated feature extraction methods, which was important in my project, considering the number of images I needed to process.
 &lt;img src=&quot;/assets/images/hog_image_example.png&quot; alt=&quot;HOG Image Example&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Assign Labels to Data:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;I assigned seven labels to the data, corresponding to seven different emotion classes:
 [0] angry, [1] happy, [2] neutral, [3] sad, [4] disgusted, [5] fearful, [6] surprised&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Train Model&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;I trained the linear regression model, made predictions, obtained the classification report, and saved the trained model for further use, achieving an accuracy of up to 77%.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improving Model Accuracy&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;I took several steps to increase the accuracy of the model by improving the feature extraction process. For example, through experimentation, I optimized the number of orientations, block size, and cell size, all with the goal of enhancing the granularity and thereby descriptive power of the extracted features. I also investigated different normalization techniques, such as min-max scaling, although this did not discernably affect the results.&lt;/li&gt;
      &lt;li&gt;Looking ahead, instead of engineering the features myself (i.e. HOG), I intend to try more feature learning techniques–automatic feature engineering–as this would empower the machine to autonomously learn more effective representations from the raw data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 Sep 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/ml_emotion_classification/</link>
        <guid isPermaLink="true">http://localhost:4000/ml_emotion_classification/</guid>
        
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Image Processing</category>
        
      </item>
    
  </channel>
</rss>
